/*****************************************************************************/
/*                                                                           */
/*  test_codebase_cuda.txt                               TTTTTTTT SSSSSSS II */
/*                                                          TT    SS      II */
/*  By: st93642@students.tsi.lv                             TT    SSSSSSS II */
/*                                                          TT         SS II */
/*  Created: Sep 29 2025 14:28 st93642                      TT    SSSSSSS II */
/*  Updated: Sep 29 2025 14:28 st93642                                       */
/*                                                                           */
/*   Transport and Telecommunication Institute - Riga, Latvia                */
/*                       https://tsi.lv                                      */
/*****************************************************************************/

// CUDA (Compute Unified Device Architecture) program
// NVIDIA's parallel computing platform and programming model for GPUs

#include <cuda_runtime.h>
#include <device_launch_parameters.h>
#include <iostream>
#include <vector>
#include <chrono>

// Error checking macro
#define CUDA_CHECK(call) \
    do { \
        cudaError_t error = call; \
        if (error != cudaSuccess) { \
            std::cerr << "CUDA Error: " << cudaGetErrorString(error) \
                      << " at line " << __LINE__ << std::endl; \
            exit(EXIT_FAILURE); \
        } \
    } while (0)

// Kernel function - runs on GPU
__global__ void vectorAdd(const float* A, const float* B, float* C, int N) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < N) {
        C[i] = A[i] + B[i];
    }
}

// Matrix multiplication kernel
__global__ void matrixMultiply(const float* A, const float* B, float* C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < N && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < N; ++k) {
            sum += A[row * N + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}

// Reduction kernel for sum
__global__ void reduceSum(const float* input, float* output, int N) {
    extern __shared__ float sdata[];

    unsigned int tid = threadIdx.x;
    unsigned int i = blockIdx.x * blockDim.x + threadIdx.x;

    sdata[tid] = (i < N) ? input[i] : 0.0f;
    __syncthreads();

    for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) {
        if (tid < s) {
            sdata[tid] += sdata[tid + s];
        }
        __syncthreads();
    }

    if (tid == 0) {
        output[blockIdx.x] = sdata[0];
    }
}

// Texture memory example
texture<float, cudaTextureType2D, cudaReadModeElementType> texRef;

__global__ void textureKernel(float* output, int width, int height) {
    int x = blockIdx.x * blockDim.x + threadIdx.x;
    int y = blockIdx.y * blockDim.y + threadIdx.y;

    if (x < width && y < height) {
        output[y * width + x] = tex2D(texRef, x, y);
    }
}

// Constant memory
__constant__ float constData[256];

__global__ void constantMemoryKernel(float* output, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        output[i] = constData[i % 256] * 2.0f;
    }
}

// Atomic operations kernel
__global__ void histogramKernel(const unsigned char* input, int* histogram, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        atomicAdd(&histogram[input[i]], 1);
    }
}

// CUDA kernel with unified memory
__global__ void unifiedMemoryKernel(float* data, int N) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < N) {
        data[i] = sinf(data[i]) + cosf(data[i]);
    }
}

// Host function to run vector addition
void runVectorAdd(int N) {
    size_t size = N * sizeof(float);

    // Allocate host memory
    float* h_A = new float[N];
    float* h_B = new float[N];
    float* h_C = new float[N];

    // Initialize host arrays
    for (int i = 0; i < N; ++i) {
        h_A[i] = static_cast<float>(i);
        h_B[i] = static_cast<float>(i * 2);
    }

    // Allocate device memory
    float *d_A, *d_B, *d_C;
    CUDA_CHECK(cudaMalloc(&d_A, size));
    CUDA_CHECK(cudaMalloc(&d_B, size));
    CUDA_CHECK(cudaMalloc(&d_C, size));

    // Copy data from host to device
    CUDA_CHECK(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice));

    // Launch kernel
    int threadsPerBlock = 256;
    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;
    vectorAdd<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);

    // Check for kernel launch errors
    CUDA_CHECK(cudaGetLastError());

    // Copy result back to host
    CUDA_CHECK(cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost));

    // Verify result
    bool success = true;
    for (int i = 0; i < N; ++i) {
        if (fabs(h_C[i] - (h_A[i] + h_B[i])) > 1e-5) {
            success = false;
            break;
        }
    }

    std::cout << "Vector addition " << (success ? "PASSED" : "FAILED") << std::endl;

    // Free device memory
    CUDA_CHECK(cudaFree(d_A));
    CUDA_CHECK(cudaFree(d_B));
    CUDA_CHECK(cudaFree(d_C));

    // Free host memory
    delete[] h_A;
    delete[] h_B;
    delete[] h_C;
}

// Host function to run matrix multiplication
void runMatrixMultiply(int N) {
    size_t size = N * N * sizeof(float);

    // Allocate host memory
    float* h_A = new float[N * N];
    float* h_B = new float[N * N];
    float* h_C = new float[N * N];

    // Initialize matrices
    for (int i = 0; i < N * N; ++i) {
        h_A[i] = static_cast<float>(rand()) / RAND_MAX;
        h_B[i] = static_cast<float>(rand()) / RAND_MAX;
    }

    // Allocate device memory
    float *d_A, *d_B, *d_C;
    CUDA_CHECK(cudaMalloc(&d_A, size));
    CUDA_CHECK(cudaMalloc(&d_B, size));
    CUDA_CHECK(cudaMalloc(&d_C, size));

    // Copy data to device
    CUDA_CHECK(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice));

    // Launch kernel
    dim3 threadsPerBlock(16, 16);
    dim3 blocksPerGrid((N + 15) / 16, (N + 15) / 16);
    matrixMultiply<<<blocksPerGrid, threadsPerBlock>>>(d_A, d_B, d_C, N);

    CUDA_CHECK(cudaGetLastError());
    CUDA_CHECK(cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost));

    std::cout << "Matrix multiplication completed" << std::endl;

    // Cleanup
    CUDA_CHECK(cudaFree(d_A));
    CUDA_CHECK(cudaFree(d_B));
    CUDA_CHECK(cudaFree(d_C));
    delete[] h_A;
    delete[] h_B;
    delete[] h_C;
}

// Host function to demonstrate streams
void runStreamsExample(int N) {
    const int numStreams = 4;
    cudaStream_t streams[numStreams];

    // Create streams
    for (int i = 0; i < numStreams; ++i) {
        CUDA_CHECK(cudaStreamCreate(&streams[i]));
    }

    size_t size = N * sizeof(float);
    float *d_A, *d_B, *d_C;
    CUDA_CHECK(cudaMalloc(&d_A, size));
    CUDA_CHECK(cudaMalloc(&d_B, size));
    CUDA_CHECK(cudaMalloc(&d_C, size));

    // Launch kernels in different streams
    int chunkSize = N / numStreams;
    for (int i = 0; i < numStreams; ++i) {
        int offset = i * chunkSize;
        vectorAdd<<<(chunkSize + 255) / 256, 256, 0, streams[i]>>>(
            d_A + offset, d_B + offset, d_C + offset, chunkSize);
    }

    // Synchronize all streams
    for (int i = 0; i < numStreams; ++i) {
        CUDA_CHECK(cudaStreamSynchronize(streams[i]));
        CUDA_CHECK(cudaStreamDestroy(streams[i]));
    }

    CUDA_CHECK(cudaFree(d_A));
    CUDA_CHECK(cudaFree(d_B));
    CUDA_CHECK(cudaFree(d_C));

    std::cout << "Streams example completed" << std::endl;
}

// Host function to demonstrate unified memory
void runUnifiedMemoryExample(int N) {
    size_t size = N * sizeof(float);

    // Allocate unified memory
    float* data;
    CUDA_CHECK(cudaMallocManaged(&data, size));

    // Initialize data on host
    for (int i = 0; i < N; ++i) {
        data[i] = static_cast<float>(i) * 3.14159f / N;
    }

    // Launch kernel
    int threadsPerBlock = 256;
    int blocksPerGrid = (N + threadsPerBlock - 1) / threadsPerBlock;
    unifiedMemoryKernel<<<blocksPerGrid, threadsPerBlock>>>(data, N);

    // Wait for GPU to finish
    CUDA_CHECK(cudaDeviceSynchronize());

    // Access data on host (automatically migrated)
    std::cout << "Unified memory example: data[0] = " << data[0] << std::endl;

    CUDA_CHECK(cudaFree(data));
}

// Main function
int main(int argc, char* argv[]) {
    std::cout << "CUDA Programming Example" << std::endl;
    std::cout << "========================" << std::endl;

    // Get device properties
    cudaDeviceProp prop;
    CUDA_CHECK(cudaGetDeviceProperties(&prop, 0));
    std::cout << "Device: " << prop.name << std::endl;
    std::cout << "Compute capability: " << prop.major << "." << prop.minor << std::endl;
    std::cout << "Total global memory: " << prop.totalGlobalMem / (1024 * 1024) << " MB" << std::endl;
    std::cout << "Max threads per block: " << prop.maxThreadsPerBlock << std::endl;

    const int N = 1000000; // 1M elements

    // Run examples
    auto start = std::chrono::high_resolution_clock::now();

    std::cout << "\nRunning vector addition..." << std::endl;
    runVectorAdd(N);

    std::cout << "Running matrix multiplication..." << std::endl;
    runMatrixMultiply(512);

    std::cout << "Running streams example..." << std::endl;
    runStreamsExample(N);

    std::cout << "Running unified memory example..." << std::endl;
    runUnifiedMemoryExample(1000);

    auto end = std::chrono::high_resolution_clock::now();
    std::chrono::duration<double> elapsed = end - start;

    std::cout << "\nAll CUDA examples completed in " << elapsed.count() << " seconds" << std::endl;

    // Reset device
    CUDA_CHECK(cudaDeviceReset());

    return EXIT_SUCCESS;
}

// Example of CUDA-accelerated library function
void cudaAcceleratedFunction(float* input, float* output, int size) {
    // This would typically be part of a CUDA library like cuBLAS, cuFFT, etc.
    // For demonstration, we'll use a simple kernel

    float *d_input, *d_output;
    size_t bytes = size * sizeof(float);

    CUDA_CHECK(cudaMalloc(&d_input, bytes));
    CUDA_CHECK(cudaMalloc(&d_output, bytes));

    CUDA_CHECK(cudaMemcpy(d_input, input, bytes, cudaMemcpyHostToDevice));

    // Launch kernel (simplified)
    int threadsPerBlock = 256;
    int blocksPerGrid = (size + threadsPerBlock - 1) / threadsPerBlock;
    unifiedMemoryKernel<<<blocksPerGrid, threadsPerBlock>>>(d_input, size);

    CUDA_CHECK(cudaMemcpy(output, d_output, bytes, cudaMemcpyDeviceToHost));

    CUDA_CHECK(cudaFree(d_input));
    CUDA_CHECK(cudaFree(d_output));
}

/*
Compilation commands:
nvcc -o cuda_example cuda_example.cu
nvcc -arch=sm_75 -o cuda_example cuda_example.cu  # For specific architecture
nvcc -gencode arch=compute_75,code=sm_75 -o cuda_example cuda_example.cu

For debugging:
nvcc -g -G -o cuda_example cuda_example.cu
cuda-gdb ./cuda_example

For profiling:
nvprof ./cuda_example
nvvp  # Visual profiler
*/
